# <center> k近邻法

## 本章思路

1. k近邻模型三要素
   - 距离度量
   - k值选择
   - 分类决策规则

2. k近邻算法
3. k近邻算法的实现: kd树

## k近邻模型三要素

### 距离度量

$$
L_p(x_i,x_j)=(x_i,x_j)=(\stackrel{n}{\underset{l=1}{\sum}}|x_i^{(l)}-x_j^{(l)}|^p)^\frac{1}{p}
$$
- $p=1$时: 曼哈顿距离
- $p=2$时: 欧氏距离
- $p=\infty$时: 各坐标距离的最大值

### k值选择

1. 较小的k: 
   - 学习的近似误差会减小，只与与输入实例较近的训练实例会对结果起作用。
   - 预测结果会对近邻的实例点非常敏感，学习的估计误差会增大，容易过拟合

2. 较大的k:
   - 减小学习的估计误差
   - 学习的近似误差会增大

较小的k模型较为复杂，较大的k模型较为简单

应用中，k值一般取较小的数值，使用交叉验证法来选取最优k值。

### 分类决策规则

多数表决: 由输入实例的k各近邻的训练实例中的多数类决定输入实例的类。

解释:

如果分类的损失函数为0-1损失函数，分类函数为
$$
f:\mathbf{R}^n \rightarrow \{c_1,c_2,\cdots,c_K\}
$$
那么误分类的概率是
$$
P(Y \ne f(X))=1-P(Y=f(X))
$$

对于给定的实例$x \in \mathcal{X}$，其最近邻的$k$个训练实例点构成的集合$N_k(x)$。如果涵盖$N_k(x)$的区域类别是$c_j$，那么误分类率是
$$
\frac{1}{k}\underset{x_i \in N_k(x)}{\sum}I(y_i \ne c_j)=1-\frac{1}{k}\underset{x_i \in N_k(x)}{\sum}I(y_i=c_j)
$$

要使误分类率最小，要使$\frac{1}{k}\underset{x_i \in N_k(x)}{\sum}I(y_i=c_j)$最大。

## kd树

存储训练数据，减少计算距离的次数。

kd树是二叉树，用垂直于坐标轴的超平面将k维空间划分，构成一系列的k维超矩形区域。

生成算法:

输入: k维空间数据集$T=\{x_1,x_2,\cdots,x_N\}$，其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(k)})^T,i=1,2,\cdots,N$

输出: kd树

1. 构造根节点，对应于包含T的k维空间的超矩形区域
   以$x^{(1)}$为坐标轴，以$x^{(1)}$坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。
2. 重复该过程
3. 直到两个子区域没有实例时停止

搜索算法:

输入: 已构造的kd树，目标点$x$

输出: $x$的最近邻

1. 在kd树中找出包含目标点$x$的叶节点: 从根节点出发，递归地向下访问kd树。用类似二叉搜索树的方法选择左右节点
2. 此叶节点为当前最近点
3. 递归向上回退
   - 如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为当前最近点
   - 当前最近点一定存在于该节点一个子节点对应的区域。检查该子节点的父节点的另一子节点对应的区域是否有更近的点。
4. 回退到根节点时搜索结束。最后的当前最近点为最近邻点

该算法复杂度为$O(\text{log}N)$，更适用于训练实例树远大于空间维数时的k近邻搜索。