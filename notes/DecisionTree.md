# <center> 决策树

## 本章基本思路

1. 决策树模型的基本概念
   - 定义
   - if-then规则
   - 条件概率分布

2. 特征选择
   - 信息增益
   - 信息增益比

3. 决策树生成算法
   - ID3算法
   - C4.5算法

4. 决策树的剪枝
5. CART算法

## 决策树模型的基本概念

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型: 内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

### if-then规则

由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。

if-then规则集合具有一个重要的性质: **互斥并且完备**

- 每一个实例都被一条路径或一条规则所覆盖
- 每一个实例只被一条路径或一条规则所覆盖

### 条件概率分布

定义在特征空间的一个划分上。将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布构成了一个条件概率分布。

决策树的一条路径对应于划分中的一个单元。

条件概率分布$P(Y|X)$由各个单元给定条件下类的条件概率分布组成。
- $X$为表示特征的随机变量，取值于给定划分下单元的集合
- $Y$为表示类的随机变量，取值于类的集合

### 决策树学习

给定训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
其中，$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$为输入实例，$n$为特征个数，$y_i \in \{1,2,\cdots,K\}$为类标记，$i=1,2,\cdots,N$，N为样本容量。

决策树的**学习目标**: 
- 根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。
- 需要有较好的泛化能力

**损失函数**: 通常为正则化的极大似然函数

**特征选择**: 在学习开始时，对特征进行选择，只留下对训练数据有足够分类能力的特征。

**基本过程**
- 构建根结点，将所有训练数据放在根结点
- 选择一个最优特征，按照这一特征将训练数据集分割成子集，使各个子集有一个在当前条件下的最好分类
- 如果这些子集已经能够被基本正确分类，构建叶结点，将这些子集分到对应的叶结点中去；如果还有自己不能被基本正确分类，那么对这些子集选择新的最优特征，继续分割
- 递归进行，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止

**剪枝**: 去掉过于细分的叶结点，使其退回到父结点，甚至更高的结点，将父结点或更高的结点改为新的叶结点

决策树的生成只考虑局部最优；决策树的剪枝则考虑全局最优

## 特征选择

### 信息增益

如果$X$是一个取有限个值的离散随机变量，其概率分布为
$$
P(X=x_i)=p_i, \quad i=1,2,\cdots,n
$$
则随机变量$X$的熵定义为
$$
H(X)=-\stackrel{n}{\underset{i=1}{\sum}}p_i \text{log} p_i
$$

设有随机变量$(X,Y)$，其联合概率分布为
$$
P(X=x_i,Y=y_i)=p_{ij}, \quad i=1,2,\cdots,n; \quad j=1,2,\cdots,m
$$
条件熵定义为
$$
H(Y|X)=\stackrel{n}{\underset{i=1}{\sum}}p_i H(Y|X=x_i)
$$
此处，$p_i=P(X=x_i), i=1,2,\cdots,n$

**信息增益**: 特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即
$$
g(D,A)=H(D)-H(D|A)
$$
信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选择方法是: 对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

定义一些变量
- 训练数据D，$|D|$表示其样本容量
- $C_k$表示第$k$个类，$k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本个数，$\stackrel{K}{\underset{k=1}{\sum}}|C_k|=|D|$
- 特征A有$n$个取值$\{a_1,a_2,\cdots,a_n\}$，根据该特征将D划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$为$D_i$的样本个数，$\stackrel{n}{\underset{i=1}{\sum}}|D_i|=|D|$
- 子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$，即$D_ik=D_i \cap C_k$，$|D_{ik}|$为$D_{ik}$的样本个数

计算信息增益
- 计算数据集$D$的经验熵$H(D)$
  
  $D$的取值范围为$C$中的类别，对应的概率值为取到对应类别的概率。此概率使用极大似然估计得到
  $$
  H(D)=-\stackrel{K}{\underset{k=1}{\sum}}\frac{|C_k|}{|D|}\text{log}_2\frac{|C_k|}{|D|}
  $$

- 计算特征$A$对数据集$D$的经验条件熵$H(D|A)$
  
  此处对应的概率值为$A$的分布
  $$
  H(D|A)=\stackrel{n}{\underset{i=1}{\sum}}\frac{|D_i|}{|D|}H(D_i)=-\stackrel{n}{\underset{i=1}{\sum}}\frac{|D_i|}{|D|}\stackrel{K}{\underset{k=1}{\sum}}\frac{|D_{ik}|}{|D_i|}\text{log}_2\frac{|D_{ik}|}{|D_i|}
  $$

- 计算信息增益
  $$
  g(D,A)=H(D)-H(D|A)
  $$

**存在的问题**: 偏向于选择取值较多的特征

**信息增益比**: 特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$
其中，$H_A(D)=-\stackrel{n}{\underset{i=1}{\sum}}\frac{|D_i|}{|D|}\text{log}_2\frac{|D_i|}{|D|}$，$n$是特征$A$取值的个数

## 决策树的生成

### ID3算法

在决策树各个结点上应用信息增益准则选择特征，**递归**地构建决策树。

算法步骤

输入: 训练数据集$D$，特征$A$阈值$\epsilon$

输出: 决策树$T$

- 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$
- 若$A=\empty$，则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
- 若$A \ne \empty$，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$
- 如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$
- 否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$，将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$
- 对第$i$个结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用以上步骤，得到子树$T_i$，返回$T_i$

### C4.5算法

将信息增益改为信息增益比即可

## 决策树的剪枝

将已生成的树进行简化的过程称为剪枝，往往通过极小化决策树整体的损失函数或代价函数来实现。

设树$T$的结点个数为$|T|$，$t$是树$T$的叶结点，该叶结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$k=1,2,\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\alpha \leqslant 0$为参数，则决策树学习的损失函数可以定义为
$$
C_{\alpha}(T)=\stackrel{|T|}{\underset{t=1}{\sum}}N_tH_t(T)+\alpha|T|
$$
其中经验熵为
$$
H_t(T)=-\underset{k}{\sum}\frac{N_{tk}}{N_t}\text{log}\frac{N_{tk}}{N_t}
$$
代入得
$$
C(T)=\stackrel{|T|}{\underset{t=1}{\sum}}N_tH_t(T)=-\stackrel{|T|}{\underset{t=1}{\sum}}\stackrel{K}{\underset{k=1}{\sum}}N_{tk}\text{log}\frac{N_{tk}}{N_t}
$$
有
$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$
- $C(T)$表示模型对训练数据的预测误差
- $|T|$表示模型的复杂度
- 参数$\alpha \geqslant 0$控制两者之间的影响，较大的$\alpha$促使选择较简单的模型

剪枝操作: 在$\alpha$确定时，选择损失函数最小的模型

算法步骤:

输入: 生成算法产生的整个树$T$，参数$\alpha$

输出: 修剪后的子树$T_{\alpha}$

- 计算每个结点的经验熵
- 递归地从树的叶结点向上回缩，如果一组叶结点被剪枝后损失减小，则将父结点变为新的叶结点
- 重复上述步骤，直至不能继续为止

## CART算法

在给定输入随机变量$X$条件下输出随机变量$Y$的条件概率分布的学习方法。假设决策树是**二叉树**

### CART生成

递归地构建二叉决策树。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则。

1. 回归树的生成

假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集
$$
D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}
$$
一棵回归树对应着输入空间的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，回归树模型可以表示为
$$
f(x)=\stackrel{M}{\underset{m=1}{\sum}}c_m I(x \in R_m)
$$
当输入空间的划分确定时，使用平方误差$\underset{x_i \in R_m}{\sum}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准则额求解每个单元上的最优输出值。

单元$R_m$上的$c_m$的最优值$\hat{c}_m$是$R_m$上的所有输入实例$x_i$对应的输出$y_i$的均值，即
$$
\hat{c}_m=\text{ave}(y_i|x_i \in R_m)
$$

**使用启发式方法对输入空间进行划分**

选择第$j$个变量$x^{(j)}$和它取的值$s$，作为切分变量和切分点，并定义两个区域
$$
R_1(j,s)=\{x|x^{(j)} \leqslant s\}, \quad R_2(j,s)=\{x|x^{(j)}>s\}
$$
寻找最优切分变量和最优切分点，即求解
$$
\underset{j,s}{\text{min}}[\underset{c_1}{\text{min}}\underset{x_i \in R_1(j,s)}{\sum}(y_i-c_1)^2+\underset{c_2}{\text{min}}\underset{x_i \in R_2(j,s)}{\sum}(y_i-c_2)^2]
$$
对固定输入变量$j$可以找到最优切分点$s$
$$
\hat{c}_1=\text{ave}(y_i|x_i \in R_1(j,s)) \quad \hat{c}_2=\text{ave}(y_i|x_i \in R_2(j,s))
$$
遍历所有输入变量，找到最优的切分变量$j$，构成一个对$(j,s)$，依次将输入空间划分为两个区域。不断重复这一过程

2. 分类树的生成

**基尼系数**: 分类问题中，假设有$K$个类，样本点属于第$k$个类的概率为$p_k$，则概率分布的基尼指数定义为
$$
\text{Gini}(p)=\stackrel{K}{\underset{k=1}{\sum}}p_k(1-p_k)=1-\stackrel{K}{\underset{k=1}{\sum}}p_k^2
$$

算法步骤

输入: 训练数据集$D$，停止计算的条件

输出: CART决策树

- 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，计算$A=a$时的基尼指数。
- 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼系数最小的特征及其对应的切分点作为最优特征和最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中
- 对两个子结点递归调用上述步骤
- 生成CART决策树

### CART剪枝

- 从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成子树序列$\{T_0,T_1,\cdots,T_n\}$
  
  计算子树的损失函数
$$
C_{\alpha}(T)=C(T)+\alpha|T|
$$
对于固定的$\alpha$，这一最优子树是唯一的

- 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树

利用独立的验证数据集，测试子树序列$T_0,T_1,\cdots,T_n$中各棵子树的平方误差和基尼指数。