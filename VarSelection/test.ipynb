{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载MNIST数据集\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')\n",
    "X /= 255.0\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 初始化RFF特征映射\n",
    "gamma = 1.0\n",
    "n_components = 1000\n",
    "rff = RBFSampler(gamma=gamma, n_components=n_components, random_state=42)\n",
    "X_train_rff = rff.fit_transform(X_train)\n",
    "X_test_rff = rff.transform(X_test)\n",
    "\n",
    "# 标准化RFF特征\n",
    "scaler = StandardScaler()\n",
    "X_train_rff = scaler.fit_transform(X_train_rff)\n",
    "X_test_rff = scaler.transform(X_test_rff)\n",
    "\n",
    "# 将数据转换为NumPy数组\n",
    "X_train_rff = np.array(X_train_rff, dtype=np.float32)\n",
    "X_test_rff = np.array(X_test_rff, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.int64)\n",
    "y_test = np.array(y_test, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_rff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 2s 1ms/step - loss: 2.4304 - accuracy: 0.1039\n",
      "Epoch 2/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 2.2344 - accuracy: 0.1778\n",
      "Epoch 3/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 2.1591 - accuracy: 0.2183\n",
      "Epoch 4/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 2.0344 - accuracy: 0.2807\n",
      "Epoch 5/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.8766 - accuracy: 0.3432\n",
      "Epoch 6/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.7272 - accuracy: 0.4018\n",
      "Epoch 7/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.6018 - accuracy: 0.4498\n",
      "Epoch 8/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.5008 - accuracy: 0.4847\n",
      "Epoch 9/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.4167 - accuracy: 0.5143\n",
      "Epoch 10/10\n",
      "1750/1750 [==============================] - 2s 1ms/step - loss: 1.3464 - accuracy: 0.5407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义神经网络模型\n",
    "# class NeuralNetwork(tf.keras.Model):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(NeuralNetwork, self).__init__()\n",
    "#         self.hidden = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "#         self.output_layer = tf.keras.layers.Dense(output_dim)\n",
    "#         self.softmax = tf.keras.layers.Dense(output_dim, activation='softmax')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.hidden(x)\n",
    "#         x = self.output_layer(x)\n",
    "#         x = self.softmax(x)\n",
    "#         x = np.argmax(x)\n",
    "#         return x\n",
    "\n",
    "def creat_model(hidden_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        # Flatten(input_shape=[, input_dim]),\n",
    "        Dense(hidden_dim, activation='relu'),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "    \n",
    "# 初始化模型\n",
    "input_dim = n_components\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = creat_model(hidden_dim, output_dim)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=optimizer, loss=loss_fn , metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(X_train_rff, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# 在测试集上评估模型\n",
    "# test_loss, test_accuracy = model.evaluate(X_test_rff, y_test, verbose=0)\n",
    "# print(f'Accuracy on test set: {test_accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - 0s 869us/step\n"
     ]
    }
   ],
   "source": [
    "pred = np.argmax(model.predict(X_test_rff), axis=1)\n",
    "true_value = 0\n",
    "for i in range(pred.shape[0]):\n",
    "    if pred[i] == y_test[i]:\n",
    "        true_value += 1\n",
    "accuracy = true_value / pred.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10678571428571429\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
